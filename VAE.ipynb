{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "from Utils import save_large_dataset, load_large_dataset\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = load_large_dataset('images')\n",
    "Y = load_large_dataset('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792, 121, 145, 3)\n"
     ]
    }
   ],
   "source": [
    "X = X.squeeze() #remove unnecessary dimension\n",
    "Y = Y.squeeze()\n",
    "\n",
    "X = X[:,:,:,58:61] #take only 3 slices and treat them as channels\n",
    "\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792, 3, 121, 145)\n"
     ]
    }
   ],
   "source": [
    "X = np.rollaxis(X, 3, 1) #move channel dimension to be the first one\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(9999) #seed fixed for reproducibility\n",
    "mask = np.random.rand(len(X)) < 0.9  #array of boolean variables\n",
    "\n",
    "training_set = X[mask]\n",
    "training_labels = Y[mask]\n",
    "\n",
    "validation_set = X[~mask]\n",
    "validation_labels = Y[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = torch.from_numpy(training_set) #convert to torch tensor\n",
    "training_labels = torch.from_numpy(training_labels) #convert to torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_labels = training_labels.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(training_set, training_labels)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LATENT_DIM = 10 #size of the latent space in the variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # layers for encoder\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*7*9, LATENT_DIM)\n",
    "        self.fc2 = nn.Linear(32*7*9, LATENT_DIM)\n",
    "        \n",
    "        \n",
    "        # layers for decoder\n",
    "        self.fc_decoder = nn.Linear(LATENT_DIM, 32*7*9)\n",
    "        \n",
    "        self.conv1_decoder = nn.Conv2d(32, 32, kernel_size=3, padding=1) \n",
    "        self.conv2_decoder = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv3_decoder = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv4_decoder = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv5_decoder = nn.Conv2d(32, 3, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.conv1(x)) #shape after conv: (8, 121, 145)\n",
    "        x = F.max_pool2d(x, kernel_size=2) #shape after pooling: (8, 60, 72)\n",
    "        \n",
    "        x = F.relu(self.conv2(x)) #shape after conv: (16, 60, 72)\n",
    "        x = F.max_pool2d(x, kernel_size=2) #shape after pooling: (16, 30, 36)\n",
    "        \n",
    "        x = F.relu(self.conv3(x)) #shape after conv: (32, 30, 36)\n",
    "        x = F.max_pool2d(x, kernel_size=2) #shape after pooling: (32, 15, 18)\n",
    "        \n",
    "        x = F.relu(self.conv4(x)) #shape after conv: (32, 15, 18)\n",
    "        x = F.max_pool2d(x, kernel_size=2) #shape after pooling: (32, 7, 9)\n",
    "        \n",
    "        x = x.view(-1, 32*7*9)\n",
    "        return self.fc1(x), self.fc2(x)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = Variable(std.data.new(std.size()).normal_())\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = F.relu(self.fc_decoder(z))\n",
    "        z = z.view(-1, 32,7,9) #reshape to (32, 7, 9)\n",
    "        \n",
    "        z = F.relu(self.conv1_decoder(z)) #shape after conv (32, 7, 9)\n",
    "        z = F.upsample(z, size=(14,18), mode='nearest') #shape after upsampling (32, 14, 18)\n",
    "        \n",
    "        z = F.relu(self.conv2_decoder(z)) #shape after conv (32, 14, 18)\n",
    "        z = F.upsample(z, size=(28,36), mode='nearest') #shape after upsampling (32, 28, 36)\n",
    "        \n",
    "        z = F.relu(self.conv3_decoder(z)) #shape after conv (32, 28, 36)\n",
    "        z = F.upsample(z, size=(56,72), mode='nearest') #shape after conv (32, 56, 72)\n",
    "        \n",
    "        z = self.conv4_decoder(z) #shape after conv (32, 56, 72)\n",
    "        z = F.upsample(z, size=(112,144), mode='nearest') #shape after conv (32, 112, 144)\n",
    "        \n",
    "        z = self.conv5_decoder(z) #shape after conv (3, 112, 144)\n",
    "        z = F.pad(z, (0,1,4,5), \"constant\", -10) #after padding (3, 121, 145) (to match the input size)\n",
    "        \n",
    "        return F.sigmoid(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(reconstruced_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(reconstruced_x.view(-1, 121*145*3), x.view(-1, 121*145*3), size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    loss = BCE + KLD\n",
    "    return loss, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = VAE()\n",
    "if (CUDA):\n",
    "    net.cuda()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstructed_images = torch.zeros(22,3,121,145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(150):  # loop over the dataset multiple times\n",
    "    \n",
    "    running_loss = 0.0 #total loss\n",
    "    running_BCE = 0.0 #reconstruction loss\n",
    "    running_KLD = 0.0 #divergence loss\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        if (CUDA):\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "                   \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        reconstructed_batch, mu, logvar = net(inputs)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss, BCE, KLD = loss_function(reconstructed_batch, inputs, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]*inputs.size(0)\n",
    "        running_BCE += BCE.data[0]*inputs.size(0)\n",
    "        running_KLD += KLD.data[0]*inputs.size(0)\n",
    "\n",
    "    print('Epoch %d, Total loss: %.3f' % (epoch + 1, running_loss / len(training_set)))\n",
    "    print('Epoch %d, BCE loss: %.3f' % (epoch + 1, running_BCE / len(training_set)))\n",
    "    print('Epoch %d, KLD loss: %.3f' % (epoch + 1, running_KLD / len(training_set)))\n",
    "    print('------------')\n",
    "    \n",
    "    \n",
    "    if (epoch%10==0): #saving examples of reconstructed images every 10 epochs\n",
    "        output = net(Variable(train_loader.dataset[0][0].unsqueeze(0).cuda()))\n",
    "        output = output[0].data.view(3,121,145).cpu()\n",
    "        reconstructed_images[int(epoch/10)] = output\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving weights\n",
    "torch.save(net.state_dict(), \"VAE_150_epochs.pt\") #weights after 150 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading weights\n",
    "net.load_state_dict(torch.load(\"VAE_150_epochs.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "augmented_images = np.zeros((len(training_set)*5, 3, 121, 145)) #5 new images for each original image\n",
    "augmented_labels = np.zeros(len(training_set)*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(training_set)):\n",
    "    for j in range(5):\n",
    "        mu, logvar = net.encode(Variable(train_loader.dataset[i][0].unsqueeze(0).cuda())) #Encoder\n",
    "        \n",
    "        std = logvar.mul(0.5).exp_() \n",
    "        eps = Variable(std.data.new(std.size()).normal_()) \n",
    "        \n",
    "        output = net.decode(eps.mul(std).add_(mu)) #Decoder\n",
    "        \n",
    "        augmented_images[(5*i)+j] = output[0].data.cpu()\n",
    "        augmented_labels[(5*i)+j] = train_loader.dataset[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_large_dataset(\"augmented_train_set_1\", augmented_images)\n",
    "save_large_dataset(\"augmented_train_set_labels_1\", augmented_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
