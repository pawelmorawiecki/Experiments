{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms, models, datasets\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "from Utils import save_large_dataset, load_large_dataset, calculate_metric, PAC2018Dataset\n",
    "import pandas as pd\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MRI images and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orginal dataset was loaded with nibabel library and saved into files. See data_loader.ipynb file for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = load_large_dataset('images')\n",
    "Y = load_large_dataset('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792, 121, 145, 121)\n"
     ]
    }
   ],
   "source": [
    "X = X.squeeze() #remove unnecessary dimension\n",
    "Y = Y.squeeze()\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final model we use only scanner ID as the extra covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('PAC2018_Sites.csv', delimiter=';')\n",
    "scanner_id = df.values[:,1:2]\n",
    "extra_features = np.zeros((len(Y),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(Y)):\n",
    "    if (scanner_id[i]==1): extra_features[i][0]=1\n",
    "    if (scanner_id[i]==2): extra_features[i][1]=1\n",
    "    if (scanner_id[i]==3): extra_features[i][2]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extra_features = extra_features.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 2D-convnet architecture, namely Resnet-18 with pretrained weights. Last layer (fully-connected classifier) was enhanced and 3 extra layers are added along with dropout. According to our experiments, it gives a bit better result than the original Resnet-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Resnet_extra_features(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Resnet_extra_features, self).__init__()\n",
    "                self.features_from_Resnet = nn.Sequential(*list(resnet_pretrained.children())[:-1])\n",
    "                \n",
    "                self.classifier = nn.Sequential(\n",
    "                nn.Linear(num_ftrs+3, 4096), #input is features of an image plus 3 extra features (scan id)\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 2),\n",
    "        )\n",
    "                \n",
    "            def forward(self, image, extra_features):\n",
    "                x = self.features_from_Resnet(image)\n",
    "                x = x.view(x.size(0), -1) #flatten\n",
    "                concatenated = torch.cat((x, extra_features), 1)\n",
    "                z = self.classifier(concatenated)\n",
    "                return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original 3d images were sliced into 2d images, each one with 3 channels (\"slices\"). We consider 2d images in all 3 dimensions. Regions, which obtained accuracy higher than 61% were selected to the final ensemble model. Here is the list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_classifiers = [\"1_22_25\", \"1_43_46\", \"1_49_52\", \"1_52_55\", \"1_58_61\", \"1_70_73\", \"1_85_88\", \"1_88_91\", \"1_97_100\", \"1_100_103\",\n",
    "                    \"2_25_28\", \"2_34_37\", \"2_55_58\", \"2_73_76\", \"2_94_97\", \"2_97_100\", \n",
    "                    \"3_10_13\", \"3_13_16\", \"3_19_22\", \"3_22_25\", \"3_25_28\", \"3_34_37\", \"3_40_43\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained and validated results on 10 randomly selected validation sets and report the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric of Ensemble Model 0.6497\n",
      "Metric of Ensemble Model 0.6245\n",
      "Metric of Ensemble Model 0.6196\n",
      "Metric of Ensemble Model 0.6363\n",
      "Metric of Ensemble Model 0.6788\n",
      "Metric of Ensemble Model 0.6518\n",
      "Metric of Ensemble Model 0.6236\n",
      "Metric of Ensemble Model 0.6276\n",
      "Metric of Ensemble Model 0.5645\n",
      "Metric of Ensemble Model 0.6250\n"
     ]
    }
   ],
   "source": [
    "for r in range(10):\n",
    "\n",
    "    np.random.seed(r) \n",
    "    mask = np.random.rand(len(X)) < 0.9  #array of boolean variables\n",
    "    validation_labels = Y[~mask]\n",
    "    aggregated_preds = np.zeros(len(validation_labels))\n",
    "\n",
    "    for i in range(len(list_classifiers)):\n",
    "\n",
    "        axis = int(list_classifiers[i].split(\"_\")[0])\n",
    "        start_index = int(list_classifiers[i].split(\"_\")[1])\n",
    "        end_index = int(list_classifiers[i].split(\"_\")[2])\n",
    "\n",
    "        if (axis==1):\n",
    "            X_sliced = X[:,start_index:end_index,:,:] #take only 3 slices and treat them as channels\n",
    "            X_padded = np.pad(X_sliced,((0,0),(0,0),(40,39),(51,52)), 'constant') #pad with zeros to get 224x224\n",
    "            X_padded = np.float32(X_padded)\n",
    "            X_padded = np.rollaxis(X_padded, axis, 1) #move channel dimension to be the first one\n",
    "\n",
    "        if (axis==2):\n",
    "            X_sliced = X[:,:,start_index:end_index,:] #take only 3 slices and treat them as channels\n",
    "            X_padded = np.pad(X_sliced,((0,0),(51,52),(0,0),(51,52)), 'constant') #pad with zeros to get 224x224\n",
    "            X_padded = np.float32(X_padded)\n",
    "            X_padded = np.rollaxis(X_padded, axis, 1) #move channel dimension to be the first one\n",
    "\n",
    "        if (axis==3):\n",
    "            X_sliced = X[:,:,:,start_index:end_index] #take only 3 slices and treat them as channels\n",
    "            X_padded = np.pad(X_sliced,((0,0),(51,52),(40,39),(0,0)), 'constant') #pad with zeros to get 224x224\n",
    "            X_padded = np.float32(X_padded)\n",
    "            X_padded = np.rollaxis(X_padded, axis, 1) #move channel dimension to be the first one\n",
    "\n",
    "\n",
    "        training_images = X_padded[mask]\n",
    "        training_extra_features = extra_features[mask]\n",
    "        training_labels = Y[mask]\n",
    "\n",
    "        validation_images = X_padded[~mask]\n",
    "        validation_extra_features = extra_features[~mask]\n",
    "        validation_labels = Y[~mask]\n",
    "\n",
    "        training_images = torch.from_numpy(training_images) #convert to torch tensor\n",
    "        training_extra_features = torch.from_numpy(training_extra_features) #convert to torch tensor\n",
    "        training_labels = torch.from_numpy(training_labels) #convert to torch tensor\n",
    "\n",
    "        validation_images = torch.from_numpy(validation_images) #convert to torch tensor\n",
    "        validation_extra_features = torch.from_numpy(validation_extra_features) #convert to torch tensor\n",
    "        validation_labels = torch.from_numpy(validation_labels) #convert to torch tensor\n",
    "\n",
    "        training_labels = training_labels.long()\n",
    "        validation_labels = validation_labels.long()\n",
    "\n",
    "        dataset = PAC2018Dataset(training_images, training_extra_features, training_labels)\n",
    "        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_set = PAC2018Dataset(validation_images, validation_extra_features, validation_labels)\n",
    "        validation_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        resnet_pretrained =  models.resnet18(pretrained=True)\n",
    "        num_ftrs = resnet_pretrained.fc.in_features\n",
    "        net = Resnet_extra_features()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        net.cuda()\n",
    "        optimizer = optim.Adam(net.parameters(), lr=1e-5, weight_decay=1e-3)\n",
    "\n",
    "        # TRAINING #\n",
    "\n",
    "        for epoch in range(4):  \n",
    "            net.train()\n",
    "            for j, data in enumerate(train_loader, 0):\n",
    "                    # get the inputs\n",
    "                    inputs, extra_feat, labels = data\n",
    "\n",
    "                    # wrap them in Variable\n",
    "                    if (CUDA):\n",
    "                        inputs, extra_feat, labels = Variable(inputs.cuda()), Variable(extra_feat.cuda()), Variable(labels.cuda())\n",
    "                    else:\n",
    "                        inputs, extra_feat, labels = Variable(inputs), Variable(extra_feat), Variable(labels)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward + backward + optimize\n",
    "                    outputs = net(inputs,extra_feat)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "        # EVALUATION #\n",
    "        net.eval()\n",
    "        preds = np.zeros((len(validation_labels)))\n",
    "        for z in range(len(validation_labels)):\n",
    "            output = net(Variable(val_set[z][0].unsqueeze(0).cuda()), Variable(val_set[z][1].unsqueeze(0).cuda()))\n",
    "            _, preds[z] = torch.max(output.data.cpu(), 1)\n",
    "            aggregated_preds[z] += preds[z]\n",
    "\n",
    "        #print ('Classifier ' + list_classifiers[i] + ': {:.4f}'.format(calculate_metric(preds,validation_labels)))\n",
    "\n",
    "    ensemble_preds = (aggregated_preds/len(list_classifiers))\n",
    "    ensemble_preds = np.rint(ensemble_preds) #round to closest integer (0 or 1)\n",
    "    #print (\"----------\")\n",
    "    print ('Metric of Ensemble Model {:.4f}'.format(calculate_metric(ensemble_preds,validation_labels)))\n",
    "    accuracy = (np.count_nonzero(ensemble_preds==validation_labels))/len(validation_labels)\n",
    "    #print ('Accuracy of Ensemble Model: {:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The reported metric is (sensitivity+specificity)/2. The average of our results is <b>0.63</b>. On the test set provided by the Photon group we reach <b>0.61</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things that didn't help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we tried normal data augmentation techniques (changing brightness, contrast, small distortions)\n",
    "* we used variational autoencoder to generate more images to augment the original dataset. We extend the size of dataset 5 times, yet did not observe any significant benefits.\n",
    "* we tried to build the latent represantion with variation autoencoder and then build classifier upon it. We didn't get anything better than 60% accuracy.\n",
    "* Instead of Resnet, we also experimented with VGG and obtained very similar results\n",
    "* We tried to incorporate other covariates (gender, age, brain volume) but it didn't help"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
